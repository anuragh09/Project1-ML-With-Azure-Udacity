# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
In this project we used the data from the banking institution database which contains the data about its customers including age, contact, loans and other relations with the bank.
The dataset contains 32950 rows and 20 column features. The goal of the model is to predict if the customer will subcribe for the term deposits. 

The solution was found using hyperdrive as well as another run was performed by AutoML.  
Using the Hyperdrive and logistic regression algorithm, 91.28% accuracy was achieved whereas running AutoML, accuracy of 91.71% was achieved. Among all the ML algorithms ran my AutoML VotingEnsemble model was best performing.

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**

There were 2 files that was used in hyperdrive with logistic regression steps. Train.py file was used to clean and prepare the data. 
Following steps were involved the the pipeline.

1. First I have created the TabularDataset using TabularDatasetFactory using the link https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/bankmarketing_train.csv

2. Cleaning the data was the second step where all the missing entried were removed and other feature engineering steps were taken

3. The data gathered was then split into train and test sets

4. Training the logistic regression model using arguments from the HyperDrive runs.

5. Accuracy was chosen as the primary metric

6. Before running the hyperdrive with accuracy as primary metric sampling method and early termination policy was defined. We chose RandomParameterSampling and BanditPolicy respct.

**What are the benefits of the parameter sampler you chose?**

Random sampling supports discrete and continuous hyperparameters. It supports early termination of low-performance runs.In random sampling, hyperparameter values are randomly selected from the defined search space. It is very beneficial when we dont have enough inforamtion about the hyperparameteres since its exploratory. 

**What are the benefits of the early stopping policy you chose?**

Early termination policies are required to terminate the runs which are running longer which in turns becomes operationally expensive in terms of resources used. 

Bandit policy is based on slack factor/slack amount and evaluation interval. Bandit terminates runs where the primary metric is not within the specified slack factor/slack amount compared to the best performing run.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

[![screenshot1.png](https://i.postimg.cc/NMDjkGz2/screenshot1.png)](https://postimg.cc/wytz6zH9)

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

When it came to the performance, the accuracy obtained from the AutoML was 91.71% which was comparitavely higher that the accuracy of 91.28% obtained using hyperdrive. The biggest advantage of using AutoML pipeline was that AutoML goes to many algorithms before picking up the best performing model. Using hyperdrive, we could only chose logistic regression algorithm using SKLearn. 

The differnce between accuracies can be explained by the fact that AutoML runs many algorithms before picking up the best one. So it is advantageous to know what algorithm to chose after running all possible choices.

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

There are certain improvements that can be done for future experiments :
1. The dataset  classes can be made balanced in terms of positive and negative instances
2. Other methods of sampling should be used instead of the  random sampling also different tuning and termination policies can also be used
3. Different primary metrics can be tested instead of Accuracy . Though mostly accuracy can be enough but its not always the correct measure for performance
4. AutoML run time can be extended to try different algorithms


## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**
